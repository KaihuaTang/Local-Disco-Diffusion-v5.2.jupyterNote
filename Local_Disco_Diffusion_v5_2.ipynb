{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TitleTop"
   },
   "source": [
    "# 1. Check GPU Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CreditsChTop"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 12 14:31:39 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  On   | 00000000:19:00.0 Off |                  N/A |\n",
      "| 27%   35C    P8    10W / 250W |   1047MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  On   | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 29%   39C    P8     1W / 250W |   6559MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  On   | 00000000:67:00.0 Off |                  N/A |\n",
      "| 27%   34C    P8    19W / 250W |     12MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  On   | 00000000:68:00.0 Off |                  N/A |\n",
      "| 27%   35C    P8     1W / 250W |     37MiB / 11016MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     19328      C   ...aconda3/envs/disco_diffusion/bin/python  1035MiB |\n",
      "|    1     19328      C   ...aconda3/envs/disco_diffusion/bin/python  6547MiB |\n",
      "|    3      1570      G   /usr/lib/xorg/Xorg                             9MiB |\n",
      "|    3      1644      G   /usr/bin/gnome-shell                          14MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "ECC features not supported for GPU 00000000:19:00.0.\n",
      "Treating as warning and moving on.\n",
      "All done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "#!nvidia-smi -i 0 -e 0\n",
    "nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "print(nvidiasmi_output)\n",
    "nvidiasmi_ecc_note = subprocess.run(['nvidia-smi', '-i', '0', '-e', '0'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "print(nvidiasmi_ecc_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Credits"
   },
   "source": [
    "# 2. Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LicenseTop"
   },
   "outputs": [],
   "source": [
    "#@title 1.2 Prepare Folders\n",
    "import subprocess, os, sys\n",
    "import pathlib, shutil\n",
    "\n",
    "# the following paths are same as preprocessing.py\n",
    "root_path = os.getcwd()\n",
    "initDirPath = f'{root_path}/outputs/init_images'\n",
    "outDirPath = f'{root_path}/outputs/images_out'\n",
    "model_path = f'{root_path}/outputs/models'\n",
    "\n",
    "# project directory\n",
    "PROJECT_DIR = os.path.abspath(os.getcwd())\n",
    "sys.path.append(PROJECT_DIR)\n",
    "os.chdir(f'{PROJECT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "License"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "# import all dependencies\n",
    "from utils.utils_os import *\n",
    "preprocess(model_path, PROJECT_DIR)\n",
    "\n",
    "from utils.utils_functions import *\n",
    "from utils.utils_midas import *\n",
    "from utils.utils_video import *\n",
    "from main import do_run\n",
    "\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import gc\n",
    "import io\n",
    "import math\n",
    "import timm\n",
    "from IPython import display\n",
    "import lpips\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "from glob import glob\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm.notebook import tqdm\n",
    "from CLIP import clip\n",
    "from resize_right import resize\n",
    "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from ipywidgets import Output\n",
    "import hashlib\n",
    "from functools import partial  \n",
    "from IPython.display import Image as ipyimg\n",
    "from numpy import asarray\n",
    "from einops import rearrange, repeat\n",
    "import torch, torchvision\n",
    "import time\n",
    "from omegaconf import OmegaConf\n",
    "from infer import InferenceHelper\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# If running locally, there's a good chance your env will need this in order to not crash upon np.matmul() or similar operations.\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "\n",
    "# AdaBins stuff\n",
    "MAX_ADABINS_AREA = 500000\n",
    "\n",
    "# CUDA Device\n",
    "DEVICE = torch.device('cuda:1' if (torch.cuda.is_available()) else 'cpu')\n",
    "print('Using device:', DEVICE)\n",
    "device = DEVICE # At least one of the modules expects this name..\n",
    "\n",
    "\n",
    "if torch.cuda.get_device_capability(DEVICE) == (8,0): ## A100 fix thanks to Emad\n",
    "    print('Disabling CUDNN for A100 gpu', file=sys.stderr)\n",
    "    torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChangelogTop"
   },
   "source": [
    "# 3. Task Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Changelog"
   },
   "outputs": [],
   "source": [
    "#@markdown ####**Basic Settings:**\n",
    "batch_name = 'TimeToDisco2'\n",
    "steps = 250                  # [25,50,100,150,250,500,1000]\n",
    "width_height = [768, 1280]\n",
    "clip_guidance_scale = 5000\n",
    "tv_scale =  0\n",
    "range_scale =   150\n",
    "sat_scale =   0\n",
    "cutn_batches = 4  \n",
    "skip_augs = False\n",
    "\n",
    "\n",
    "# *Init Settings:*\n",
    "# *Make sure you set skip_steps to ~50% of your steps if you want to use an init image.*\n",
    "init_image = None \n",
    "init_scale = 1000 \n",
    "skip_steps = 10 \n",
    "\n",
    "\n",
    "#Get corrected sizes\n",
    "side_x = (width_height[0]//64)*64;\n",
    "side_y = (width_height[1]//64)*64;\n",
    "if side_x != width_height[0] or side_y != width_height[1]:\n",
    "  print(f'Changing output size to {side_x}x{side_y}. Dimensions must by multiples of 64.')\n",
    "\n",
    "#Update Model Settings\n",
    "timestep_respacing = f'ddim{steps}'\n",
    "diffusion_steps = (1000//steps)*steps if steps < 1000 else steps\n",
    "\n",
    "#Make folder for batch\n",
    "batchFolder = f'{outDirPath}/{batch_name}'\n",
    "createPath(batchFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TutorialTop"
   },
   "source": [
    "# 4. Diffusion and CLIP model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DiffusionSet"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 Model already downloaded, check check_model_SHA if the file is corrupt\n",
      "Secondary Model already downloaded, check check_model_SHA if the file is corrupt\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /home/kaihua/anaconda3/envs/disco_diffusion/lib/python3.7/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "#@markdown ####**Models Settings:**\n",
    "\n",
    "diffusion_model = \"512x512_diffusion_uncond_finetune_008100\"  # \"256x256_diffusion_uncond\" / \"512x512_diffusion_uncond_finetune_008100\"\n",
    "use_secondary_model = True \n",
    "diffusion_sampling_mode = 'ddim' # 'plms' / 'ddim'  \n",
    "\n",
    "use_checkpoint = True \n",
    "ViTB32 = True \n",
    "ViTB16 = True \n",
    "ViTL14 = False\n",
    "RN101 = False \n",
    "RN50 = True \n",
    "RN50x4 = False \n",
    "RN50x16 = False \n",
    "RN50x64 = False \n",
    "\n",
    "check_model_SHA = False\n",
    "\n",
    "download_models(model_path, check_model_SHA, diffusion_model, use_secondary_model)\n",
    "\n",
    "model_config = model_and_diffusion_defaults()\n",
    "model_config = update_diffusion_config(diffusion_model, model_config, use_checkpoint, timestep_respacing, diffusion_steps)\n",
    "model_default = model_config['image_size']\n",
    "\n",
    "if use_secondary_model:\n",
    "    secondary_model = SecondaryDiffusionImageNet2()\n",
    "    secondary_model.load_state_dict(torch.load(f'{model_path}/secondary_model_imagenet_2.pth', map_location='cpu'))\n",
    "    secondary_model.eval().requires_grad_(False).to(device)\n",
    "\n",
    "clip_models = []\n",
    "if ViTB32 is True: clip_models.append(clip.load('ViT-B/32', jit=False)[0].eval().requires_grad_(False).to(device)) \n",
    "if ViTB16 is True: clip_models.append(clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device) ) \n",
    "if ViTL14 is True: clip_models.append(clip.load('ViT-L/14', jit=False)[0].eval().requires_grad_(False).to(device) ) \n",
    "if RN50 is True: clip_models.append(clip.load('RN50', jit=False)[0].eval().requires_grad_(False).to(device))\n",
    "if RN50x4 is True: clip_models.append(clip.load('RN50x4', jit=False)[0].eval().requires_grad_(False).to(device)) \n",
    "if RN50x16 is True: clip_models.append(clip.load('RN50x16', jit=False)[0].eval().requires_grad_(False).to(device)) \n",
    "if RN50x64 is True: clip_models.append(clip.load('RN50x64', jit=False)[0].eval().requires_grad_(False).to(device)) \n",
    "if RN101 is True: clip_models.append(clip.load('RN101', jit=False)[0].eval().requires_grad_(False).to(device)) \n",
    "\n",
    "lpips_model = lpips.LPIPS(net='vgg').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SetupTop"
   },
   "source": [
    "# 5. Animation Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CheckGPU"
   },
   "outputs": [],
   "source": [
    "# Animation Mode:\n",
    "#For animation, you probably want to turn `cutn_batches` to 1 to make it quicker.*\n",
    "\n",
    "animation_mode = 'None' # ['None', '2D', '3D', 'Video Input'] \n",
    "\n",
    "\n",
    "# Video Input Settings:\n",
    "video_init_path = \"training.mp4\"\n",
    "extract_nth_frame = 2 \n",
    "video_init_seed_continuity = True \n",
    "\n",
    "if animation_mode == \"Video Input\":\n",
    "  videoFramesFolder = f'videoFrames'\n",
    "  createPath(videoFramesFolder)\n",
    "  print(f\"Exporting Video Frames (1 every {extract_nth_frame})...\")\n",
    "  try:\n",
    "    for f in pathlib.Path(f'{videoFramesFolder}').glob('*.jpg'):\n",
    "      f.unlink()\n",
    "  except:\n",
    "    print('')\n",
    "  vf = f'select=not(mod(n\\,{extract_nth_frame}))'\n",
    "  subprocess.run(['ffmpeg', '-i', f'{video_init_path}', '-vf', f'{vf}', '-vsync', 'vfr', '-q:v', '2', '-loglevel', 'error', '-stats', f'{videoFramesFolder}/%04d.jpg'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "\n",
    "\n",
    "\n",
    "# 2D Animation Settings:\n",
    "# `zoom` is a multiplier of dimensions, 1 is no zoom.\n",
    "# All rotations are provided in degrees.\n",
    "\n",
    "key_frames = True \n",
    "max_frames = 10000\n",
    "\n",
    "if animation_mode == \"Video Input\":\n",
    "  max_frames = len(glob(f'{videoFramesFolder}/*.jpg'))\n",
    "\n",
    "interp_spline = 'Linear' #Do not change, currently will not look good. param ['Linear','Quadratic','Cubic']\n",
    "angle = \"0:(0)\"\n",
    "zoom = \"0: (1), 10: (1.05)\"\n",
    "translation_x = \"0: (0)\"\n",
    "translation_y = \"0: (0)\"\n",
    "translation_z = \"0: (10.0)\"\n",
    "rotation_3d_x = \"0: (0)\"\n",
    "rotation_3d_y = \"0: (0)\"\n",
    "rotation_3d_z = \"0: (0)\"\n",
    "midas_depth_model = \"dpt_large\"\n",
    "midas_weight = 0.3\n",
    "near_plane = 200\n",
    "far_plane = 10000\n",
    "fov = 40\n",
    "padding_mode = 'border'\n",
    "sampling_mode = 'bicubic'\n",
    "\n",
    "#======= TURBO MODE\n",
    "#@markdown ---\n",
    "#@markdown ####**Turbo Mode (3D anim only):**\n",
    "#@markdown (Starts after frame 10,) skips diffusion steps and just uses depth map to warp images for skipped frames.\n",
    "#@markdown Speeds up rendering by 2x-4x, and may improve image coherence between frames. frame_blend_mode smooths abrupt texture changes across 2 frames.\n",
    "#@markdown For different settings tuned for Turbo Mode, refer to the original Disco-Turbo Github: https://github.com/zippy731/disco-diffusion-turbo\n",
    "\n",
    "turbo_mode = False \n",
    "turbo_steps = \"3\"       # [\"2\",\"3\",\"4\",\"5\",\"6\"] \n",
    "turbo_preroll = 10 \n",
    "\n",
    "#insist turbo be used only w 3d anim.\n",
    "if turbo_mode and animation_mode != '3D':\n",
    "  print('=====')\n",
    "  print('Turbo mode only available with 3D animations. Disabling Turbo.')\n",
    "  print('=====')\n",
    "  turbo_mode = False\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown ####**Coherency Settings:**\n",
    "#@markdown `frame_scale` tries to guide the new frame to looking like the old one. A good default is 1500.\n",
    "#@markdown `frame_skip_steps` will blur the previous frame - higher values will flicker less but struggle to add enough new detail to zoom into.\n",
    "\n",
    "frames_scale = 1500 \n",
    "frames_skip_steps = '60%' # ['40%', '50%', '60%', '70%', '80%']\n",
    "\n",
    "#======= VR MODE\n",
    "#@markdown ---\n",
    "#@markdown ####**VR Mode (3D anim only):**\n",
    "#@markdown Enables stereo rendering of left/right eye views (supporting Turbo) which use a different (fish-eye) camera projection matrix.   \n",
    "#@markdown Note the images you're prompting will work better if they have some inherent wide-angle aspect\n",
    "#@markdown The generated images will need to be combined into left/right videos. These can then be stitched into the VR180 format.\n",
    "#@markdown Google made the VR180 Creator tool but subsequently stopped supporting it. It's available for download in a few places including https://www.patrickgrunwald.de/vr180-creator-download\n",
    "#@markdown The tool is not only good for stitching (videos and photos) but also for adding the correct metadata into existing videos, which is needed for services like YouTube to identify the format correctly.\n",
    "#@markdown Watching YouTube VR videos isn't necessarily the easiest depending on your headset. For instance Oculus have a dedicated media studio and store which makes the files easier to access on a Quest https://creator.oculus.com/manage/mediastudio/\n",
    "#@markdown \n",
    "#@markdown The command to get ffmpeg to concat your frames for each eye is in the form: `ffmpeg -framerate 15 -i frame_%4d_l.png l.mp4` (repeat for r)\n",
    "\n",
    "#@markdown `vr_eye_angle` is the y-axis rotation of the eyes towards the center\n",
    "#@markdown interpupillary distance (between the eyes)\n",
    "\n",
    "vr_mode = False\n",
    "vr_eye_angle = 0.5\n",
    "vr_ipd = 5.0\n",
    "\n",
    "#insist VR be used only w 3d anim.\n",
    "if vr_mode and animation_mode != '3D':\n",
    "  print('=====')\n",
    "  print('VR mode only available with 3D animations. Disabling VR.')\n",
    "  print('=====')\n",
    "  vr_mode = False\n",
    "    \n",
    "# parse parameters\n",
    "series_params, float_params = update_parameters(key_frames, max_frames, interp_spline, angle, zoom, translation_x, translation_y, translation_z,\n",
    "                                                  rotation_3d_x, rotation_3d_y, rotation_3d_z)\n",
    "angle_series, zoom_series, translation_x_series, translation_y_series, translation_z_series, rotation_3d_x_series, rotation_3d_y_series, rotation_3d_z_series = series_params\n",
    "angle, zoom, translation_x, translation_y, translation_z, rotation_3d_x, rotation_3d_y, rotation_3d_z = float_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "id": "PrepFolders"
   },
   "source": [
    "# 6. Extra Settings\n",
    " Partial Saves, Advanced Settings, Cutn Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "InstallDeps"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save every 21 steps\n"
     ]
    }
   ],
   "source": [
    "#@markdown ####**Saving:**\n",
    "\n",
    "#@markdown Intermediate steps will save a copy at your specified intervals. You can either format it as a single integer or a list of specific steps \n",
    "#@markdown A value of `2` will save a copy at 33% and 66%. 0 will save none.\n",
    "#@markdown A value of `[5, 9, 34, 45]` will save at steps 5, 9, 34, and 45. (Make sure to include the brackets)\n",
    "\n",
    "intermediate_saves = 10 \n",
    "intermediates_in_subfolder = True  \n",
    "\n",
    "if type(intermediate_saves) is not list:\n",
    "  if intermediate_saves:\n",
    "    steps_per_checkpoint = math.floor((steps - skip_steps - 1) // (intermediate_saves+1))\n",
    "    steps_per_checkpoint = steps_per_checkpoint if steps_per_checkpoint > 0 else 1\n",
    "    print(f'Will save every {steps_per_checkpoint} steps')\n",
    "  else:\n",
    "    steps_per_checkpoint = steps+10\n",
    "else:\n",
    "  steps_per_checkpoint = None\n",
    "\n",
    "if intermediate_saves and intermediates_in_subfolder is True:\n",
    "  partialFolder = f'{batchFolder}/partials'\n",
    "  createPath(partialFolder)\n",
    "\n",
    "\n",
    "\n",
    "#@markdown ####**Advanced Settings:**\n",
    "#@markdown *There are a few extra advanced settings available if you double click this cell.*\n",
    "\n",
    "#@markdown *Perlin init will replace your init, so uncheck if using one.*\n",
    "\n",
    "perlin_init = False  \n",
    "perlin_mode = 'mixed' # ['mixed', 'color', 'gray']\n",
    "set_seed = 'random_seed'\n",
    "eta = 0.8\n",
    "clamp_grad = True\n",
    "clamp_max = 0.05\n",
    "\n",
    "\n",
    "### EXTRA ADVANCED SETTINGS:\n",
    "randomize_class = True\n",
    "clip_denoised = False\n",
    "fuzzy_prompt = False\n",
    "rand_mag = 0.05\n",
    "\n",
    "#@markdown ####**Cutn Scheduling:**\n",
    "#@markdown Format: `[40]*400+[20]*600` = 40 cuts for the first 400 /1000 steps, then 20 for the last 600/1000\n",
    "\n",
    "#@markdown cut_overview and cut_innercut are cumulative for total cutn on any given step. Overview cuts see the entire image and are good for early structure, innercuts are your standard cutn.\n",
    "\n",
    "cut_overview = \"[12]*400+[4]*600\" \n",
    "cut_innercut =\"[4]*400+[12]*600\"\n",
    "cut_ic_pow = 1\n",
    "cut_icgray_p = \"[0.2]*400+[0]*600\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DefMidasFns"
   },
   "source": [
    "# 7. Prompts\n",
    "`animation_mode: None` will only use the first set. `animation_mode: 2D / Video` will run through them per the set frames and hold on the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DefFns"
   },
   "outputs": [],
   "source": [
    "text_prompts = {\n",
    "    0: [\"A beautiful painting of a singular lighthouse, shining its light across a dense forest by greg rutkowski and thomas kinkade, Trending on artstation.:2\", \"a girl and a boy sit around a camp fire under the stars.:2\", \"blue color scheme:1\"],\n",
    "    100: [\"This set of prompts start at frame 100\",\"This prompt has weight five:5\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DiffClipSetTop"
   },
   "outputs": [],
   "source": [
    "image_prompts = {\n",
    "    # 0:['ImagePromptsWorkButArentVeryGood.png:2',],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ModelSettings"
   },
   "source": [
    "# 8. Run Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SettingsTop"
   },
   "outputs": [],
   "source": [
    "display_rate =  25 \n",
    "n_batches =  10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BasicSettings"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnimSetTop"
   },
   "source": [
    "# 4. Diffuse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AnimSettings",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af4c444677340e19a1b0d6c4eba3694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65efda32e2fd4799815015752838fac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202478d09e1d4b3dbbc1350558ee70f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed used: 4209623164\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19328/2609253779.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m   \u001b[0mdo_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiffusion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlpips_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecondary_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/kaihua/new_project/disco_diffusion/main.py\u001b[0m in \u001b[0;36mdo_run\u001b[0;34m(args, diffusion, model, lpips_model, secondary_model, model_path, gpu_device)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# with run_display:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# display.clear_output(wait=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m               \u001b[0mcur_t\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m               \u001b[0mintermediateStep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/kaihua/new_project/disco_diffusion/guided-diffusion/guided_diffusion/gaussian_diffusion.py\u001b[0m in \u001b[0;36mddim_sample_loop_progressive\u001b[0;34m(self, model, shape, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress, eta, skip_timesteps, init_image, randomize_class, cond_fn_with_grad)\u001b[0m\n\u001b[1;32m    898\u001b[0m                     \u001b[0mcond_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcond_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m                     \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                     \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m                 )\n\u001b[1;32m    902\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/kaihua/new_project/disco_diffusion/guided-diffusion/guided_diffusion/gaussian_diffusion.py\u001b[0m in \u001b[0;36mddim_sample\u001b[0;34m(self, model, x, t, clip_denoised, denoised_fn, cond_fn, model_kwargs, eta)\u001b[0m\n\u001b[1;32m    672\u001b[0m         )\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcond_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcondition_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_orig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/kaihua/new_project/disco_diffusion/guided-diffusion/guided_diffusion/respace.py\u001b[0m in \u001b[0;36mcondition_score\u001b[0;34m(self, cond_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcondition_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcondition_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/kaihua/new_project/disco_diffusion/guided-diffusion/guided_diffusion/gaussian_diffusion.py\u001b[0m in \u001b[0;36mcondition_score\u001b[0;34m(self, cond_fn, p_mean_var, x, t, model_kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_eps_from_xstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_mean_var\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred_xstart\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         eps = eps - (1 - alpha_bar).sqrt() * cond_fn(\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale_timesteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         )\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/kaihua/new_project/disco_diffusion/guided-diffusion/guided_diffusion/respace.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, ts, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrescale_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mnew_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_ts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1000.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_num_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data1/kaihua/new_project/disco_diffusion/main.py\u001b[0m in \u001b[0;36mcond_fn\u001b[0;34m(x, t, y)\u001b[0m\n\u001b[1;32m    252\u001b[0m                               \u001b[0mInnerCrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut_innercut\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt_int\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIC_Size_Pow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut_ic_pow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIC_Grey_P\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut_icgray_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt_int\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                               )\n\u001b[0;32m--> 254\u001b[0;31m                       \u001b[0mclip_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m                       \u001b[0mimage_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_stat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"clip_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                       \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspherical_dist_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_stat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target_embeds\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/disco_diffusion/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/kaihua/new_project/disco_diffusion/utils/utils_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0moutput_shape_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mpad_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msideY\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msideY\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msideX\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msideX\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0mcutout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/disco_diffusion/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_pad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   4383\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_pad_circular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4384\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4385\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4387\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Update Model Settings\n",
    "timestep_respacing = f'ddim{steps}'\n",
    "diffusion_steps = (1000//steps)*steps if steps < 1000 else steps\n",
    "model_config.update({\n",
    "    'timestep_respacing': timestep_respacing,\n",
    "    'diffusion_steps': diffusion_steps,\n",
    "})\n",
    "\n",
    "batch_size = 1 \n",
    "\n",
    "\n",
    "\n",
    "resume_run = False\n",
    "run_to_resume = 'latest'\n",
    "resume_from_frame = 'latest'\n",
    "retain_overwritten_frames = False\n",
    "if retain_overwritten_frames is True:\n",
    "  retainFolder = f'{batchFolder}/retained'\n",
    "  createPath(retainFolder)\n",
    "\n",
    "\n",
    "skip_step_ratio = int(frames_skip_steps.rstrip(\"%\")) / 100\n",
    "calc_frames_skip_steps = math.floor(steps * skip_step_ratio)\n",
    "\n",
    "\n",
    "if steps <= calc_frames_skip_steps:\n",
    "  sys.exit(\"ERROR: You can't skip more steps than your total steps\")\n",
    "\n",
    "if resume_run:\n",
    "  if run_to_resume == 'latest':\n",
    "    try:\n",
    "      batchNum\n",
    "    except:\n",
    "      batchNum = len(glob(f\"{batchFolder}/{batch_name}(*)_settings.txt\"))-1\n",
    "  else:\n",
    "    batchNum = int(run_to_resume)\n",
    "  if resume_from_frame == 'latest':\n",
    "    start_frame = len(glob(batchFolder+f\"/{batch_name}({batchNum})_*.png\"))\n",
    "    if animation_mode != '3D' and turbo_mode == True and start_frame > turbo_preroll and start_frame % int(turbo_steps) != 0:\n",
    "      start_frame = start_frame - (start_frame % int(turbo_steps))\n",
    "  else:\n",
    "    start_frame = int(resume_from_frame)+1\n",
    "    if animation_mode != '3D' and turbo_mode == True and start_frame > turbo_preroll and start_frame % int(turbo_steps) != 0:\n",
    "      start_frame = start_frame - (start_frame % int(turbo_steps))\n",
    "    if retain_overwritten_frames is True:\n",
    "      existing_frames = len(glob(batchFolder+f\"/{batch_name}({batchNum})_*.png\"))\n",
    "      frames_to_save = existing_frames - start_frame\n",
    "      print(f'Moving {frames_to_save} frames to the Retained folder')\n",
    "      move_files(start_frame, existing_frames, batchFolder, retainFolder, batch_name, batchNum)\n",
    "else:\n",
    "  start_frame = 0\n",
    "  batchNum = len(glob(batchFolder+\"/*.txt\"))\n",
    "  while os.path.isfile(f\"{batchFolder}/{batch_name}({batchNum})_settings.txt\") is True or os.path.isfile(f\"{batchFolder}/{batch_name}-{batchNum}_settings.txt\") is True:\n",
    "    batchNum += 1\n",
    "\n",
    "print(f'Starting Run: {batch_name}({batchNum}) at frame {start_frame}')\n",
    "\n",
    "if set_seed == 'random_seed':\n",
    "    random.seed()\n",
    "    seed = random.randint(0, 2**32)\n",
    "    # print(f'Using seed: {seed}')\n",
    "else:\n",
    "    seed = int(set_seed)\n",
    "\n",
    "args = {\n",
    "    'batchNum': batchNum,\n",
    "    'prompts_series':split_prompts(text_prompts, max_frames) if text_prompts else None,\n",
    "    'image_prompts_series':split_prompts(image_prompts, max_frames) if image_prompts else None,\n",
    "    'seed': seed,\n",
    "    'display_rate':display_rate,\n",
    "    'n_batches':n_batches if animation_mode == 'None' else 1,\n",
    "    'batch_size':batch_size,\n",
    "    'batch_name': batch_name,\n",
    "    'steps': steps,\n",
    "    'diffusion_sampling_mode': diffusion_sampling_mode,\n",
    "    'width_height': width_height,\n",
    "    'clip_guidance_scale': clip_guidance_scale,\n",
    "    'tv_scale': tv_scale,\n",
    "    'range_scale': range_scale,\n",
    "    'sat_scale': sat_scale,\n",
    "    'cutn_batches': cutn_batches,\n",
    "    'init_image': init_image,\n",
    "    'init_scale': init_scale,\n",
    "    'skip_steps': skip_steps,\n",
    "    'side_x': side_x,\n",
    "    'side_y': side_y,\n",
    "    'timestep_respacing': timestep_respacing,\n",
    "    'diffusion_steps': diffusion_steps,\n",
    "    'animation_mode': animation_mode,\n",
    "    'video_init_path': video_init_path,\n",
    "    'extract_nth_frame': extract_nth_frame,\n",
    "    'video_init_seed_continuity': video_init_seed_continuity,\n",
    "    'key_frames': key_frames,\n",
    "    'max_frames': max_frames if animation_mode != \"None\" else 1,\n",
    "    'interp_spline': interp_spline,\n",
    "    'start_frame': start_frame,\n",
    "    'angle': angle,\n",
    "    'zoom': zoom,\n",
    "    'translation_x': translation_x,\n",
    "    'translation_y': translation_y,\n",
    "    'translation_z': translation_z,\n",
    "    'rotation_3d_x': rotation_3d_x,\n",
    "    'rotation_3d_y': rotation_3d_y,\n",
    "    'rotation_3d_z': rotation_3d_z,\n",
    "    'midas_depth_model': midas_depth_model,\n",
    "    'midas_weight': midas_weight,\n",
    "    'near_plane': near_plane,\n",
    "    'far_plane': far_plane,\n",
    "    'fov': fov,\n",
    "    'padding_mode': padding_mode,\n",
    "    'sampling_mode': sampling_mode,\n",
    "    'angle_series':angle_series,\n",
    "    'zoom_series':zoom_series,\n",
    "    'translation_x_series':translation_x_series,\n",
    "    'translation_y_series':translation_y_series,\n",
    "    'translation_z_series':translation_z_series,\n",
    "    'rotation_3d_x_series':rotation_3d_x_series,\n",
    "    'rotation_3d_y_series':rotation_3d_y_series,\n",
    "    'rotation_3d_z_series':rotation_3d_z_series,\n",
    "    'frames_scale': frames_scale,\n",
    "    'calc_frames_skip_steps': calc_frames_skip_steps,\n",
    "    'skip_step_ratio': skip_step_ratio,\n",
    "    'calc_frames_skip_steps': calc_frames_skip_steps,\n",
    "    'text_prompts': text_prompts,\n",
    "    'image_prompts': image_prompts,\n",
    "    'cut_overview': eval(cut_overview),\n",
    "    'cut_innercut': eval(cut_innercut),\n",
    "    'cut_ic_pow': cut_ic_pow,\n",
    "    'cut_icgray_p': eval(cut_icgray_p),\n",
    "    'intermediate_saves': intermediate_saves,\n",
    "    'intermediates_in_subfolder': intermediates_in_subfolder,\n",
    "    'steps_per_checkpoint': steps_per_checkpoint,\n",
    "    'perlin_init': perlin_init,\n",
    "    'perlin_mode': perlin_mode,\n",
    "    'set_seed': set_seed,\n",
    "    'eta': eta,\n",
    "    'clamp_grad': clamp_grad,\n",
    "    'clamp_max': clamp_max,\n",
    "    'skip_augs': skip_augs,\n",
    "    'randomize_class': randomize_class,\n",
    "    'clip_denoised': clip_denoised,\n",
    "    'fuzzy_prompt': fuzzy_prompt,\n",
    "    'rand_mag': rand_mag,\n",
    "    'resume_run': resume_run,\n",
    "    'batchFolder': batchFolder,\n",
    "    'batch_name': batch_name,\n",
    "    'batchNum': batchNum,\n",
    "    'turbo_mode': turbo_mode,\n",
    "    'turbo_preroll': turbo_preroll,\n",
    "    'turbo_steps': turbo_steps,\n",
    "    'vr_mode': vr_mode,\n",
    "    'video_init_seed_continuity': video_init_seed_continuity,\n",
    "    'videoFramesFolder': videoFramesFolder if animation_mode == \"Video Input\" else None,\n",
    "    'clip_models': clip_models,\n",
    "    'use_secondary_model': use_secondary_model,\n",
    "    'partialFolder': partialFolder,\n",
    "}\n",
    "\n",
    "args = SimpleNamespace(**args)\n",
    "\n",
    "print('Prepping model...')\n",
    "model, diffusion = create_model_and_diffusion(**model_config)\n",
    "model.load_state_dict(torch.load(f'{model_path}/{diffusion_model}.pt', map_location='cpu'))\n",
    "model.requires_grad_(False).eval().to(device)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
    "        param.requires_grad_()\n",
    "if model_config['use_fp16']:\n",
    "    model.convert_to_fp16()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "try:\n",
    "  do_run(args, diffusion, model,lpips_model, secondary_model, model_path, gpu_device=device)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    print('Seed used:', seed)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExtraSetTop"
   },
   "source": [
    "# 5. Create the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExtraSettings"
   },
   "outputs": [],
   "source": [
    "# @title ### **Create video**\n",
    "#@markdown Video file will save in the same folder as your images.\n",
    "\n",
    "skip_video_for_run_all = False #@param {type: 'boolean'}\n",
    "\n",
    "if skip_video_for_run_all == True:\n",
    "  print('Skipping video creation, uncheck skip_video_for_run_all if you want to run it')\n",
    "\n",
    "else:\n",
    "  # import subprocess in case this cell is run without the above cells\n",
    "  import subprocess\n",
    "  from base64 import b64encode\n",
    "\n",
    "  latest_run = batchNum\n",
    "\n",
    "  folder = batch_name #@param\n",
    "  run = latest_run #@param\n",
    "  final_frame = 'final_frame'\n",
    "\n",
    "\n",
    "  init_frame = 1#@param {type:\"number\"} This is the frame where the video will start\n",
    "  last_frame = final_frame#@param {type:\"number\"} You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n",
    "  fps = 12#@param {type:\"number\"}\n",
    "  # view_video_in_cell = True #@param {type: 'boolean'}\n",
    "\n",
    "  frames = []\n",
    "  # tqdm.write('Generating video...')\n",
    "\n",
    "  if last_frame == 'final_frame':\n",
    "    last_frame = len(glob(batchFolder+f\"/{folder}({run})_*.png\"))\n",
    "    print(f'Total frames: {last_frame}')\n",
    "\n",
    "  image_path = f\"{outDirPath}/{folder}/{folder}({run})_%04d.png\"\n",
    "  filepath = f\"{outDirPath}/{folder}/{folder}({run}).mp4\"\n",
    "\n",
    "\n",
    "  cmd = [\n",
    "      'ffmpeg',\n",
    "      '-y',\n",
    "      '-vcodec',\n",
    "      'png',\n",
    "      '-r',\n",
    "      str(fps),\n",
    "      '-start_number',\n",
    "      str(init_frame),\n",
    "      '-i',\n",
    "      image_path,\n",
    "      '-frames:v',\n",
    "      str(last_frame+1),\n",
    "      '-c:v',\n",
    "      'libx264',\n",
    "      '-vf',\n",
    "      f'fps={fps}',\n",
    "      '-pix_fmt',\n",
    "      'yuv420p',\n",
    "      '-crf',\n",
    "      '17',\n",
    "      '-preset',\n",
    "      'veryslow',\n",
    "      filepath\n",
    "  ]\n",
    "\n",
    "  process = subprocess.Popen(cmd, cwd=f'{batchFolder}', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "  stdout, stderr = process.communicate()\n",
    "  if process.returncode != 0:\n",
    "      print(stderr)\n",
    "      raise RuntimeError(stderr)\n",
    "  else:\n",
    "      print(\"The video is ready and saved to the images folder\")\n",
    "\n",
    "  # if view_video_in_cell:\n",
    "  #     mp4 = open(filepath,'rb').read()\n",
    "  #     data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "  #     display.HTML(f'<video width=400 controls><source src=\"{data_url}\" type=\"video/mp4\"></video>')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PromptsTop"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Prompts"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiffuseTop"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoTheRun"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CreateVidTop"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CreateVid"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "CreditsChTop",
    "TutorialTop",
    "CheckGPU",
    "InstallDeps",
    "DefMidasFns",
    "DefFns",
    "DefSecModel",
    "DefSuperRes",
    "AnimSetTop",
    "ExtraSetTop"
   ],
   "machine_shape": "hm",
   "name": "Disco Diffusion v5.2 [w/ VR Mode]",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
